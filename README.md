Perfeito, Keniel ğŸ‘Œ â€” segue abaixo o **README.md atualizado e completo**, agora com a **seÃ§Ã£o inicial â€œğŸ§­ Como Participar do Experimentoâ€** explicando exatamente **como os participantes devem criar o fork, clonar, commitar e enviar o cÃ³digo**, tudo no mesmo tom profissional e padronizado.

VocÃª pode usar esse arquivo diretamente como README principal do repositÃ³rio **template**.

---


# ğŸ§­ Como Participar do Experimento

Bem-vindo(a)! ğŸ‰  
VocÃª foi selecionado para participar do experimento **"AvaliaÃ§Ã£o do Impacto de Ferramentas de InteligÃªncia Artificial na GeraÃ§Ã£o de CÃ³digo"**.

O objetivo Ã© analisar **como ferramentas de IA influenciam produtividade e qualidade de cÃ³digo** em comparaÃ§Ã£o com o desenvolvimento tradicional.

Siga atentamente as instruÃ§Ãµes abaixo.  
Cada etapa Ã© essencial para garantir a validade cientÃ­fica do estudo. ğŸ’¡

---

## ğŸ§© Etapas de ParticipaÃ§Ã£o

### 1. **Crie seu fork do repositÃ³rio template**
1. Acesse o repositÃ³rio base:  
   ğŸ‘‰ [[https://github.com/ia-code-experiment/template-experiment-ai](https://github.com/kenielnunes/pfc-experiment-ai-code-template)]
2. Clique em **Fork â†’ Create fork**.
3. Nomeie seu repositÃ³rio da seguinte forma:
```

pfc-experiment-ai-code-template-seu-nome

````
Exemplo: `pfc-experiment-ai-code-template-joaosilva`

---

### 2. **Clone o seu fork para a sua mÃ¡quina**
```bash
git clone https://github.com/pfc-experiment-ai-code-template/pfc-experiment-ai-code-template-joaosilva.git
cd pfc-experiment-ai-code-template-joaosilva
npm install
````

---

### 3. **Implemente as tarefas**

* As trÃªs tarefas estÃ£o nas pastas:

  ```
  /tarefa1-validate-user
  /tarefa2-refactor-code
  /tarefa3-todo-manager
  ```
* Leia o `README.md` dentro de cada pasta antes de comeÃ§ar.
* Desenvolva o cÃ³digo e garanta que os testes passem (`npm test`).

---

### 4. **Commits e organizaÃ§Ã£o**

Durante o desenvolvimento:

* FaÃ§a **commits frequentes e claros** (a cada parte concluÃ­da).
* Use a convenÃ§Ã£o abaixo:

| Prefixo     | Uso                              |
| ----------- | -------------------------------- |
| `feat:`     | nova funcionalidade              |
| `fix:`      | correÃ§Ã£o de bug                  |
| `refactor:` | melhoria sem mudar comportamento |
| `test:`     | criaÃ§Ã£o/alteraÃ§Ã£o de testes      |
| `docs:`     | alteraÃ§Ã£o em documentaÃ§Ã£o        |
| `chore:`    | manutenÃ§Ã£o ou setup              |

ğŸ“Œ **Exemplos:**

```
feat: adiciona validaÃ§Ã£o de CPF
fix: corrige cÃ¡lculo de idade mÃ­nima
refactor: separa funÃ§Ã£o de validaÃ§Ã£o de senha
```

> ğŸ’¡ Recomenda-se fazer **1 commit a cada 15â€“30 minutos** de progresso.

---

### 5. **Envie o cÃ³digo (push)**

ApÃ³s cada commit:

```bash
git push origin main
```

Isso enviarÃ¡ seu cÃ³digo para o GitHub, acionando automaticamente o **SonarCloud** para anÃ¡lise de qualidade.

---

### 6. **Regras de conduta**

* Use **somente a ferramenta de IA designada ao seu grupo** (Claude, ChatGPT, Gemini ou nenhuma no grupo controle).
* NÃ£o compartilhe seu cÃ³digo com outros participantes.
* NÃ£o use soluÃ§Ãµes externas (StackOverflow, GitHub etc.).
* Ã‰ permitido usar prompts para compreender ou gerar cÃ³digo relacionado Ã  tarefa.
* Mantenha o cÃ³digo pÃºblico para que as mÃ©tricas possam ser coletadas.

---

### 7. **ApÃ³s finalizar todas as tarefas**

* Verifique se todos os testes (`npm test`) passam.
* Confirme que os commits estÃ£o no GitHub.
* Responda o **QuestionÃ¡rio PÃ³s-Experimento** enviado por e-mail.

---

# ğŸ§  Experimento: Impacto de Ferramentas de IA na GeraÃ§Ã£o de CÃ³digo

Este repositÃ³rio faz parte do estudo **"AnÃ¡lise Comparativa do Impacto de Ferramentas de InteligÃªncia Artificial para GeraÃ§Ã£o de CÃ³digo na Produtividade e Qualidade do Desenvolvimento de Software"**.

O objetivo deste experimento Ã© **avaliar a influÃªncia de ferramentas de IA** (como ChatGPT, Claude e Gemini) no processo de desenvolvimento de software, comparando com o desenvolvimento tradicional.

---

## ğŸ“‹ Estrutura do RepositÃ³rio

Cada participante deverÃ¡ desenvolver **trÃªs tarefas independentes**, localizadas nas pastas:

```
/tarefa1-validate-user
/tarefa2-refactor-code
/tarefa3-todo-manager
```

Cada pasta contÃ©m:

* Um arquivo `README.md` com a descriÃ§Ã£o detalhada da tarefa;
* Arquivo(s) `.js` com o cÃ³digo base a ser implementado;
* Pasta `/tests` com os testes automatizados;
* Tempo limite estimado para conclusÃ£o.

---

## âš™ï¸ PreparaÃ§Ã£o do Ambiente

### 1. Requisitos

* **Node.js 20+**
* **npm** (instalado junto com Node)
* **VS Code** ou **Cursor IDE**
* Acesso Ã  internet (para usar a ferramenta de IA, se aplicÃ¡vel)

### 2. InstalaÃ§Ã£o

Clone ou baixe o repositÃ³rio (ou o **fork** que vocÃª recebeu):

```bash
git clone <seu-repositorio-fork-url>
cd experiment-ai-template
npm install
```

---

## ğŸš€ ExecuÃ§Ã£o das Tarefas

### ğŸ”¹ Etapas obrigatÃ³rias:

1. **Leia o README** de cada tarefa antes de iniciar.
2. **Implemente o cÃ³digo** dentro da pasta indicada.
3. **Garanta que os testes passam** com sucesso (`npm test`).
4. **FaÃ§a commits frequentes**, com mensagens claras.
5. **NÃ£o use cÃ³digo pronto da internet** (StackOverflow, GitHub etc.).
   Use **apenas a ferramenta de IA designada** e o seu conhecimento.

---

## ğŸ§© DescriÃ§Ã£o das Tarefas

### ğŸ§ª Tarefa 1 â€” ValidaÃ§Ã£o de FormulÃ¡rio

Implementar um sistema simples de validaÃ§Ã£o de dados de usuÃ¡rio.

* ValidaÃ§Ã£o de email, CPF, senha e idade.
* Tratamento de erros personalizados.
* Retornar objeto com status e mensagens.

â± Tempo estimado: **1h30**

---

### ğŸ§  Tarefa 2 â€” RefatoraÃ§Ã£o de CÃ³digo

Refatorar um cÃ³digo legado propositalmente ruim, mantendo a mesma saÃ­da.

* Melhorar nomes de variÃ¡veis e modularidade.
* Remover duplicaÃ§Ãµes.
* Adicionar tratamento de erros.

â± Tempo estimado: **2h**

---

### âš™ï¸ Tarefa 3 â€” Gerenciador Local de Tarefas (sem API)

Implementar um sistema de gerenciamento de tarefas em memÃ³ria.

* Classe `TodoManager` com CRUD simples.
* ValidaÃ§Ã£o de dados e tratamento de erros.
* PaginaÃ§Ã£o local e login simulado.

â± Tempo estimado: **3h**

---

## ğŸ’¬ Regras do Experimento

1. Cada participante pertence a **um grupo especÃ­fico**:

   * **GC** â†’ Grupo Controle (sem IA)
   * **GE1** â†’ Claude
   * **GE2** â†’ ChatGPT
   * **GE3** â†’ Gemini

2. Os grupos **experimentais** devem usar **apenas a ferramenta de IA designada** para auxiliar no desenvolvimento.

3. Ã‰ permitido:

   * Usar prompts para entender requisitos ou gerar cÃ³digo **relacionado Ã  tarefa**.
   * Ajustar, refatorar e comentar o cÃ³digo livremente.

4. Ã‰ proibido:

   * Consultar repositÃ³rios externos prontos.
   * Compartilhar cÃ³digo entre participantes.
   * Utilizar mÃºltiplas ferramentas de IA (apenas a designada).

5. Todos os commits devem ser feitos com mensagens claras, por exemplo:

   ```
   feat: implementa validaÃ§Ã£o de senha
   fix: corrige verificaÃ§Ã£o de CPF
   refactor: extrai funÃ§Ã£o auxiliar
   ```

---

## ğŸ” AvaliaÃ§Ã£o AutomÃ¡tica (SonarCloud)

Este repositÃ³rio estÃ¡ configurado com **SonarCloud** via GitHub Actions.
A cada `push`, serÃ¡ feita uma anÃ¡lise automÃ¡tica de qualidade de cÃ³digo.

As mÃ©tricas coletadas incluem:

* **Complexidade ciclomÃ¡tica**
* **Code smells**
* **Bugs**
* **Vulnerabilidades**
* **DuplicaÃ§Ã£o de cÃ³digo**
* **Ãndice de manutenibilidade**

VocÃª pode acompanhar sua anÃ¡lise diretamente no painel do SonarCloud:
ğŸ‘‰ [https://sonarcloud.io/projects](https://sonarcloud.io/projects)

---

## ğŸ§¾ EntregÃ¡veis

Cada tarefa serÃ¡ avaliada com base em:

| Categoria              | DescriÃ§Ã£o                                    | Peso |
| ---------------------- | -------------------------------------------- | ---- |
| âœ… Funcionalidade       | Cumprimento dos requisitos e testes passando | 30%  |
| ğŸ§© Qualidade de CÃ³digo | Clareza, organizaÃ§Ã£o e boas prÃ¡ticas (Sonar) | 30%  |
| ğŸ§  Produtividade       | Tempo e nÃºmero de commits (Git)              | 20%  |
| ğŸ’¬ PercepÃ§Ã£o           | Feedback pÃ³s-experimento (questionÃ¡rio)      | 20%  |

---

## ğŸ“¦ Scripts DisponÃ­veis

| Comando            | DescriÃ§Ã£o                             |
| ------------------ | ------------------------------------- |
| `npm install`      | Instala dependÃªncias                  |
| `npm test`         | Executa todos os testes automatizados |
| `npm run coverage` | Gera relatÃ³rio de cobertura de testes |

---

## ğŸ§ª Workflow AutomÃ¡tico (CI/CD)

O repositÃ³rio contÃ©m uma pipeline automÃ¡tica via **GitHub Actions**:

```yaml
name: SonarCloud
on: [push]
jobs:
  analyze:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-node@v3
        with:
          node-version: 20
      - run: npm ci
      - run: npm test -- --coverage
      - uses: SonarSource/sonarcloud-github-action@v2
        env:
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
```

Essa pipeline executa os testes e envia automaticamente as mÃ©tricas para o SonarCloud.

---

## ğŸ§‘â€ğŸ’» Contato e Suporte

Se tiver dÃºvidas durante o experimento:

* Entre em contato com o pesquisador responsÃ¡vel **(Keniel Alves Nunes)**.
* Ou envie mensagem no canal oficial do experimento no **Discord/WhatsApp**.

---

## ğŸ“… Importante

* O tempo total estimado para todas as tarefas Ã© de **6 a 7 horas**.
* As tarefas devem ser concluÃ­das dentro do **prazo combinado**.
* ApÃ³s finalizar, responda ao **QuestionÃ¡rio PÃ³s-Experimento** enviado por email.

---

### ğŸ’¡ Lembre-se:

> O objetivo nÃ£o Ã© apenas â€œterminar o cÃ³digoâ€, mas **mostrar como vocÃª usa raciocÃ­nio e ferramentas de IA para resolver problemas de programaÃ§Ã£o com qualidade**.

Boa sorte e bom cÃ³digo! ğŸš€
```
